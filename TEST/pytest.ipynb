{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d8c3be-df30-4c46-a627-c1d8fd4d63f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bring in your ETL notebook\n",
    "%run /Workspace/energyconsumption/db_ingestion\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ca0068-1c00-48c3-bcce-f54e8d88ee14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Repos/databricks/awsproject/db_refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9cb188-598c-4c64-995e-9fc6edebdacd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#this path is working\n",
    "%run /Workspace/Users/venkynani701@gmail.com/curated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ae2b31f-a77b-43b0-87af-409b937a2f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec82dd2-5c0e-4b35-ba40-ca80a0729147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#test_db_ingestion\n",
    "\n",
    "# Use %run to include the db_ingestion notebook at the top of your test notebook\n",
    "# %run ./db_ingestion\n",
    "\n",
    "import pytest\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# --------------------------\n",
    "# Spark fixture\n",
    "# --------------------------\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .master(\"local[2]\")\n",
    "        .appName(\"pytest-db-ingestion\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark\n",
    "\n",
    "# --------------------------\n",
    "# Test log_step()\n",
    "# --------------------------\n",
    "def test_log_step(spark):\n",
    "    print(\"\\n=== Running test_log_step ===\")\n",
    "    row = Row(\n",
    "        job_id=\"12345\",\n",
    "        timestamp=\"2025-08-25T12:00:00\",\n",
    "        step=\"TEST_STEP\",\n",
    "        status=\"INFO\",\n",
    "        message=\"This is a test log\"\n",
    "    )\n",
    "    df = spark.createDataFrame([row])\n",
    "    display(df)\n",
    "\n",
    "    assert \"step\" in df.columns\n",
    "    assert \"status\" in df.columns\n",
    "    assert \"message\" in df.columns\n",
    "    print(\"✅ log_step test passed!\")\n",
    "\n",
    "# --------------------------\n",
    "# Test row count\n",
    "# --------------------------\n",
    "def test_row_count(spark):\n",
    "    print(\"\\n=== Running test_row_count ===\")\n",
    "    sample_data = [\n",
    "        Row(date=\"2025-08-01\", consumption=10),\n",
    "        Row(date=\"2025-08-01\", consumption=20),\n",
    "        Row(date=\"2025-08-02\", consumption=30),\n",
    "    ]\n",
    "    df = spark.createDataFrame(sample_data)\n",
    "    display(df)\n",
    "\n",
    "    count = df.count()\n",
    "    print(\"Row count:\", count)\n",
    "\n",
    "    assert count == 3\n",
    "    print(\"✅ row_count test passed!\")\n",
    "\n",
    "# --------------------------\n",
    "# Test ETL failure handling\n",
    "# --------------------------\n",
    "def test_etl_failure():\n",
    "    print(\"\\n=== Running test_etl_failure ===\")\n",
    "    try:\n",
    "        raise Exception(\"S3 not reachable\")\n",
    "    except Exception as e:\n",
    "        print(\"Captured Exception:\", str(e))\n",
    "        assert \"S3 not reachable\" in str(e)\n",
    "        print(\"✅ etl_failure test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d669b1-c4da-43c3-8176-0598f81cd21d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# At the top of your test notebook, add:\n",
    "# %run /path/to/db_refinement\n",
    "\n",
    "import pytest\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Remove: import db_refinement as job\n",
    "\n",
    "# --------------------------\n",
    "# Spark fixture\n",
    "# --------------------------\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    # SparkSession is available in Databricks, so you can omit this fixture\n",
    "    pass\n",
    "\n",
    "# --------------------------\n",
    "# Test log_step()\n",
    "# --------------------------\n",
    "def test_log_step():\n",
    "    row = Row(\n",
    "        job_id=\"test123\",\n",
    "        timestamp=\"2025-08-25T12:00:00\",\n",
    "        step=\"TEST_STEP\",\n",
    "        status=\"INFO\",\n",
    "        message=\"Testing log\"\n",
    "    )\n",
    "    df = spark.createDataFrame([row])\n",
    "    display(df)\n",
    "\n",
    "    assert \"step\" in df.columns\n",
    "    assert df.collect()[0][\"status\"] == \"INFO\"\n",
    "\n",
    "# --------------------------\n",
    "# Test numeric casting\n",
    "# --------------------------\n",
    "def test_numeric_casting():\n",
    "    sample_data = [\n",
    "        Row(Date=\"1/1/2007\", Time=\"00:00:00\", Global_active_power=\"1.23\"),\n",
    "        Row(Date=\"1/1/2007\", Time=\"00:01:00\", Global_active_power=\"?\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(sample_data)\n",
    "\n",
    "    df_casted = df.withColumn(\n",
    "        \"Global_active_power\",\n",
    "        F.when(F.col(\"Global_active_power\") == \"?\", None)\n",
    "         .otherwise(F.col(\"Global_active_power\"))\n",
    "         .cast(DoubleType())\n",
    "    )\n",
    "\n",
    "    display(df_casted)\n",
    "\n",
    "    values = [row[\"Global_active_power\"] for row in df_casted.collect()]\n",
    "    assert values[0] == 1.23\n",
    "    assert values[1] is None\n",
    "\n",
    "# --------------------------\n",
    "# Test timestamp transformation\n",
    "# --------------------------\n",
    "def test_timestamp_creation():\n",
    "    sample_data = [\n",
    "        Row(Date=\"1/1/2007\", Time=\"00:00:00\"),\n",
    "        Row(Date=\"1/1/2007\", Time=\"12:30:45\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(sample_data)\n",
    "\n",
    "    df_ts = df.withColumn(\n",
    "        \"timestamp\",\n",
    "        F.to_timestamp(F.concat_ws(\" \", F.col(\"Date\"), F.col(\"Time\")), \"d/M/yyyy H:m:s\")\n",
    "    )\n",
    "\n",
    "    display(df_ts)\n",
    "\n",
    "    timestamps = [row[\"timestamp\"] for row in df_ts.collect()]\n",
    "    assert timestamps[0] is not None\n",
    "    assert str(timestamps[1]).endswith(\"12:30:45\")\n",
    "\n",
    "# --------------------------\n",
    "# Test deduplication\n",
    "# --------------------------\n",
    "def test_deduplication():\n",
    "    sample_data = [\n",
    "        Row(timestamp=\"2025-08-25 00:00:00\", value=10),\n",
    "        Row(timestamp=\"2025-08-25 00:00:00\", value=10),\n",
    "        Row(timestamp=\"2025-08-25 00:01:00\", value=20),\n",
    "    ]\n",
    "    df = spark.createDataFrame(sample_data)\n",
    "\n",
    "    df_dedup = df.dropDuplicates([\"timestamp\"])\n",
    "    display(df_dedup)\n",
    "\n",
    "    count = df_dedup.count()\n",
    "    assert count == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc920db-946a-4a72-b95d-45a6d7c89536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# At the top of your test notebook, add:\n",
    "# %run /path/to/db_curateddata\n",
    "\n",
    "import pytest\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Remove: import db_curateddata as job\n",
    "\n",
    "# --------------------------\n",
    "# Test log_step()\n",
    "# --------------------------\n",
    "def test_log_step():\n",
    "    row = Row(\n",
    "        job_id=\"pytest123\",\n",
    "        timestamp=\"2025-08-25T12:00:00\",\n",
    "        step=\"CURATED_TEST\",\n",
    "        status=\"INFO\",\n",
    "        message=\"Testing curated log\"\n",
    "    )\n",
    "    df = spark.createDataFrame([row])\n",
    "    display(df)\n",
    "\n",
    "    assert \"step\" in df.columns\n",
    "    assert df.collect()[0][\"status\"] == \"INFO\"\n",
    "\n",
    "# --------------------------\n",
    "# Test daily aggregation\n",
    "# --------------------------\n",
    "def test_daily_aggregation():\n",
    "    sample_data = [\n",
    "        Row(timestamp=\"2025-08-25 00:00:00\", Global_active_power=1.2, Voltage=220.0,\n",
    "            Sub_metering_1=10, Sub_metering_2=20, Sub_metering_3=5),\n",
    "        Row(timestamp=\"2025-08-25 01:00:00\", Global_active_power=0.8, Voltage=221.0,\n",
    "            Sub_metering_1=15, Sub_metering_2=25, Sub_metering_3=10),\n",
    "    ]\n",
    "    df = spark.createDataFrame(sample_data)\n",
    "\n",
    "    daily_agg_df = df.groupBy(\n",
    "        F.to_date(\"timestamp\").alias(\"date\")\n",
    "    ).agg(\n",
    "        F.sum(\"Global_active_power\").alias(\"total_active_power_kw\"),\n",
    "        F.avg(\"Voltage\").alias(\"avg_voltage\"),\n",
    "        F.sum(\"Sub_metering_1\").alias(\"total_sub_metering_1_wh\"),\n",
    "        F.sum(\"Sub_metering_2\").alias(\"total_sub_metering_2_wh\"),\n",
    "        F.sum(\"Sub_metering_3\").alias(\"total_sub_metering_3_wh\"),\n",
    "    )\n",
    "\n",
    "    display(daily_agg_df)\n",
    "\n",
    "    result = daily_agg_df.collect()[0]\n",
    "    assert round(result[\"total_active_power_kw\"], 1) == 2.0\n",
    "    assert result[\"total_sub_metering_1_wh\"] == 25\n",
    "\n",
    "# --------------------------\n",
    "# Test peak hour calculation\n",
    "# --------------------------\n",
    "def test_peak_hour():\n",
    "    sample_data = [\n",
    "        Row(timestamp=\"2025-08-25 00:00:00\", Global_active_power=1.2),\n",
    "        Row(timestamp=\"2025-08-25 01:00:00\", Global_active_power=5.0),\n",
    "        Row(timestamp=\"2025-08-25 02:00:00\", Global_active_power=2.0),\n",
    "    ]\n",
    "    df = spark.createDataFrame(sample_data)\n",
    "\n",
    "    hourly_consumption = df.groupBy(\n",
    "        F.to_date(\"timestamp\").alias(\"date\"),\n",
    "        F.hour(\"timestamp\").alias(\"hour\")\n",
    "    ).agg(\n",
    "        F.sum(\"Global_active_power\").alias(\"hourly_active_power_kw\")\n",
    "    )\n",
    "\n",
    "    window_spec = Window.partitionBy(\"date\").orderBy(F.col(\"hourly_active_power_kw\").desc())\n",
    "    peak_hour_df = hourly_consumption.withColumn(\n",
    "        \"rank\", F.row_number().over(window_spec)\n",
    "    ).filter(\n",
    "        F.col(\"rank\") == 1\n",
    "    ).select(\n",
    "        \"date\", F.col(\"hour\").alias(\"peak_consumption_hour\")\n",
    "    )\n",
    "\n",
    "    display(peak_hour_df)\n",
    "\n",
    "    peak_hour = peak_hour_df.collect()[0][\"peak_consumption_hour\"]\n",
    "    assert peak_hour == 1  # Hour 01:00 has highest power\n",
    "\n",
    "# --------------------------\n",
    "# Test final gold dataframe enrichment\n",
    "# --------------------------\n",
    "def test_final_gold_features():\n",
    "    sample_data = [\n",
    "        Row(date=\"2025-08-25\", total_active_power_kw=10.5, avg_voltage=220.5,\n",
    "            total_sub_metering_1_wh=25, total_sub_metering_2_wh=30,\n",
    "            total_sub_metering_3_wh=40, peak_consumption_hour=12)\n",
    "    ]\n",
    "    df = spark.createDataFrame(sample_data)\n",
    "\n",
    "    enriched_df = df.withColumn(\n",
    "        \"day_name\", F.date_format(\"date\", \"EEEE\")\n",
    "    ).withColumn(\n",
    "        \"day_of_week\", F.dayofweek(\"date\")\n",
    "    ).withColumn(\n",
    "        \"month\", F.month(\"date\")\n",
    "    )\n",
    "\n",
    "    display(enriched_df)\n",
    "\n",
    "    row = enriched_df.collect()[0]\n",
    "    assert \"day_of_week\" in row.asDict()\n",
    "    assert row[\"month\"] == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c4564ff-1e2e-41be-a257-dfae0591aa2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pytest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
