{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca53c690-c3fd-4b58-b408-bd62fd06efd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Import Libraries\n",
    "# --------------------------------\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import traceback\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# Initialize Spark Session\n",
    "# --------------------------------\n",
    "spark = SparkSession.builder.appName(\"EnergyConsumptionDailyGold\").getOrCreate()\n",
    "\n",
    "# --------------------------------\n",
    "# Job ID for this run\n",
    "# --------------------------------\n",
    "job_id = str(uuid.uuid4())\n",
    "\n",
    "# --------------------------------\n",
    "# Logging function (writes into Delta table)\n",
    "# --------------------------------\n",
    "def log_step(step_name, status=\"INFO\", message=\"\"):\n",
    "    log_df = spark.createDataFrame([\n",
    "        Row(\n",
    "            job_id=job_id,\n",
    "            timestamp=datetime.utcnow().isoformat(),\n",
    "            step=step_name,\n",
    "            status=status,\n",
    "            message=message\n",
    "        )\n",
    "    ])\n",
    "    (\n",
    "        log_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(\"project_logs.gold_log.log\")   # Gold log table\n",
    "    )\n",
    "\n",
    "# --------------------------------\n",
    "# Config\n",
    "# --------------------------------\n",
    "source_catalog = \"aws_refinement\"\n",
    "source_schema = \"default\"\n",
    "source_table = \"usage_cleaned\"\n",
    "\n",
    "target_catalog = \"aws_curateddata\"\n",
    "target_schema = \"default\"\n",
    "target_table_daily = \"daily_summary\"\n",
    "\n",
    "silver_table_full_name = f\"{source_catalog}.{source_schema}.{source_table}\"\n",
    "gold_table_daily_full_name = f\"{target_catalog}.{target_schema}.{target_table_daily}\"\n",
    "\n",
    "# --------------------------------\n",
    "# Gold ETL\n",
    "# --------------------------------\n",
    "try:\n",
    "    log_step(\"START\", \"INFO\", f\"Starting Daily Gold processing from {silver_table_full_name}\")\n",
    "\n",
    "    # 1. Read the cleaned Silver data\n",
    "    silver_df = spark.read.table(silver_table_full_name)\n",
    "    log_step(\"LOAD_SILVER\", \"INFO\", f\"Loaded Silver table with {silver_df.count()} records\")\n",
    "\n",
    "    # 2. Aggregate daily metrics\n",
    "    daily_agg_df = silver_df.groupBy(F.to_date(\"timestamp\").alias(\"date\")) \\\n",
    "        .agg(\n",
    "            F.sum(\"Global_active_power\").alias(\"total_active_power_kw\"),\n",
    "            F.avg(\"Voltage\").alias(\"avg_voltage\"),\n",
    "            F.sum(\"Sub_metering_1\").alias(\"total_sub_metering_1_wh\"),\n",
    "            F.sum(\"Sub_metering_2\").alias(\"total_sub_metering_2_wh\"),\n",
    "            F.sum(\"Sub_metering_3\").alias(\"total_sub_metering_3_wh\")\n",
    "        )\n",
    "    log_step(\"DAILY_AGG\", \"INFO\", \"Daily aggregation completed\")\n",
    "\n",
    "    # 3. Calculate hourly consumption\n",
    "    hourly_consumption = silver_df.groupBy(\n",
    "        F.to_date(\"timestamp\").alias(\"date\"),\n",
    "        F.hour(\"timestamp\").alias(\"hour\")\n",
    "    ).agg(\n",
    "        F.sum(\"Global_active_power\").alias(\"hourly_active_power_kw\")\n",
    "    )\n",
    "\n",
    "    # 4. Find peak consumption hour\n",
    "    window_spec = Window.partitionBy(\"date\").orderBy(F.col(\"hourly_active_power_kw\").desc())\n",
    "    peak_hour_df = hourly_consumption.withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "                                     .filter(F.col(\"rank\") == 1) \\\n",
    "                                     .select(\"date\", F.col(\"hour\").alias(\"peak_consumption_hour\"))\n",
    "    log_step(\"PEAK_HOUR\", \"INFO\", \"Peak consumption hour calculation completed\")\n",
    "\n",
    "    # 5. Join daily summary with peak hour info\n",
    "    final_gold_df = daily_agg_df.join(peak_hour_df, on=\"date\", how=\"left\") \\\n",
    "                                .withColumn(\"day_name\", F.date_format(\"date\", \"EEEE\"))\n",
    "\n",
    "    # 6. Add date features\n",
    "    final_gold_df = final_gold_df.withColumn(\"day_of_week\", F.dayofweek(\"date\")) \\\n",
    "                                 .withColumn(\"month\", F.month(\"date\")) \\\n",
    "                                 .orderBy(\"date\")\n",
    "\n",
    "    log_step(\"FINAL_PREP\", \"INFO\", \"Final Gold DataFrame prepared\")\n",
    "\n",
    "    # 7. Write to Gold Delta table\n",
    "    final_gold_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(gold_table_daily_full_name)\n",
    "\n",
    "    log_step(\"SAVE_GOLD\", \"SUCCESS\", f\"Saved Daily Gold table {gold_table_daily_full_name}\")\n",
    "    log_step(\"END\", \"SUCCESS\", \"Daily Gold processing completed\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_step(\"ERROR\", \"FAIL\", traceback.format_exc())\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b17233-03e2-4049-a2bf-7c23f520ca73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_gold_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "db_curateddata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
