{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696e528e-9ad8-4656-adb1-4cacfef390a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Import Libraries\n",
    "# --------------------------------\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import traceback\n",
    "\n",
    "# --------------------------------\n",
    "# Initialize Spark Session\n",
    "# --------------------------------\n",
    "spark = SparkSession.builder.appName(\"EnergyConsumptionSilverPipeline\").getOrCreate()\n",
    "\n",
    "# --------------------------------\n",
    "# Job ID for this run\n",
    "# --------------------------------\n",
    "job_id = str(uuid.uuid4())\n",
    "\n",
    "# --------------------------------\n",
    "# Logging function\n",
    "# --------------------------------\n",
    "def log_step(step_name, status=\"INFO\", message=\"\"):\n",
    "    log_df = spark.createDataFrame([\n",
    "        Row(\n",
    "            job_id=job_id,\n",
    "            timestamp=datetime.utcnow().isoformat(),\n",
    "            step=step_name,\n",
    "            status=status,\n",
    "            message=message\n",
    "        )\n",
    "    ])\n",
    "    (\n",
    "        log_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(\"project_logs.silver_log.log\")   # Silver log table\n",
    "    )\n",
    "\n",
    "# --------------------------------\n",
    "# Config\n",
    "# --------------------------------\n",
    "source_catalog_s3 = \"aws_dataingestion.bronze_aws.household\"\n",
    "target_catalog = \"aws_refinement\"\n",
    "target_schema = \"default\"\n",
    "target_table = \"usage_cleaned\"\n",
    "\n",
    "table_name = \"usage_records\"\n",
    "bronze_s3_path = source_catalog_s3\n",
    "silver_table_full_name = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "\n",
    "# --------------------------------\n",
    "# Silver ETL\n",
    "# --------------------------------\n",
    "try:\n",
    "    log_step(\"START\", \"INFO\", f\"Starting Silver processing for {table_name} from {bronze_s3_path}\")\n",
    "\n",
    "    # 1. Read Bronze Delta data\n",
    "    df = spark.read.table(bronze_s3_path)\n",
    "    initial_count = df.count()\n",
    "    log_step(\"LOAD\", \"INFO\", f\"Loaded {initial_count} rows from Bronze path\")\n",
    "\n",
    "    # 2. Combine Date + Time → timestamp\n",
    "    df_transformed = df.withColumn(\n",
    "        \"timestamp\",\n",
    "        F.to_timestamp(F.concat_ws(\" \", F.col(\"Date\"), F.col(\"Time\")), \"d/M/yyyy H:m:s\")\n",
    "    )\n",
    "\n",
    "    # 3. Cast numeric cols, handle '?'\n",
    "    numeric_cols = [\n",
    "        \"Global_active_power\", \"Global_reactive_power\", \"Voltage\",\n",
    "        \"Global_intensity\", \"Sub_metering_1\", \"Sub_metering_2\", \"Sub_metering_3\"\n",
    "    ]\n",
    "    for col_name in numeric_cols:\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name) == \"?\", None).otherwise(F.col(col_name)).cast(DoubleType())\n",
    "        )\n",
    "    log_step(\"CAST_NUMERIC\", \"INFO\", \"Numeric columns casted with '?' handled\")\n",
    "\n",
    "    # 4. Fill null values with defaults\n",
    "    defaults = {\n",
    "        \"Global_active_power\": 0.0, \"Global_reactive_power\": 0.0, \"Voltage\": 0.0,\n",
    "        \"Global_intensity\": 0.0, \"Sub_metering_1\": 0.0, \"Sub_metering_2\": 0.0,\n",
    "        \"Sub_metering_3\": 0.0\n",
    "    }\n",
    "    df_filled = df_transformed.fillna(defaults)\n",
    "\n",
    "    # 5. Drop rows with null timestamp\n",
    "    df_cleaned = df_filled.filter(F.col(\"timestamp\").isNotNull())\n",
    "\n",
    "    # 6. Deduplicate\n",
    "    df_final = df_cleaned.dropDuplicates([\"timestamp\"])\n",
    "    final_count = df_final.count()\n",
    "    log_step(\"DEDUP\", \"INFO\", f\"Row count after cleaning/deduplication: {final_count}\")\n",
    "\n",
    "    # 7. Write Silver Table\n",
    "    df_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_table_full_name)\n",
    "    log_step(\"SAVE\", \"SUCCESS\", f\"Saved Silver table {silver_table_full_name}\")\n",
    "\n",
    "    log_step(\"END\", \"SUCCESS\", f\"Silver processing completed ✅. Rows: {final_count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_step(\"ERROR\", \"FAIL\", traceback.format_exc())\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4833266-4ce7-453f-997c-610d011dd54c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "# Add year and month columns for partitioning\n",
    "df_final = df_final.withColumn(\"year\", year(F.col(\"timestamp\"))) \\\n",
    "                         .withColumn(\"month\", month(F.col(\"timestamp\")))\n",
    "\n",
    "# Write to Silver Delta table with partitioning\n",
    "df_final.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .saveAsTable(silver_table_full_name)\n",
    "\n",
    "logger.info(f\" Successfully saved Silver table with partitioning by year/month: {silver_table_full_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6aa3da7-baa1-4f79-addb-63618d5aba3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_silver_vacuum(spark_session):\n",
    "    silver_table_full_name = \"default.silver_test\"\n",
    "\n",
    "    # Create table if not exists\n",
    "    data = [(\"2025-08-23 10:00:00\", 100.0)]\n",
    "    df = spark_session.createDataFrame(data, [\"timestamp\", \"value\"])\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table_full_name)\n",
    "\n",
    "    # Run VACUUM\n",
    "    spark_session.sql(f\"VACUUM {silver_table_full_name} RETAIN 168 HOURS\")\n",
    "\n",
    "    # Validate table still exists\n",
    "    count = spark_session.table(silver_table_full_name).count()\n",
    "    assert count > 0\n",
    "    print(\"VACUUM test passed\")\n",
    "\n",
    "# Call test function manually\n",
    "test_silver_vacuum(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d058f8f7-8951-42fc-b9f4-7fe5c7d8c2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Manually call the function outside pytest\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or reuse Spark session\n",
    "spark = SparkSession.builder.appName(\"TestSilverOptimize\").getOrCreate()\n",
    "\n",
    "# Define the function\n",
    "def test_silver_optimize(spark_session):\n",
    "    silver_table_full_name = \"silver_test\"\n",
    "\n",
    "    # Run OPTIMIZE\n",
    "    spark_session.sql(f\"OPTIMIZE {silver_table_full_name} ZORDER BY (timestamp)\")\n",
    "\n",
    "    # Just check table is still accessible after optimize\n",
    "    count = spark_session.table(silver_table_full_name).count()\n",
    "    assert count > 0\n",
    "    print(f\" OPTIMIZE successful. Row count: {count}\")\n",
    "\n",
    "# Call the function manually\n",
    "test_silver_optimize(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5111d342-7e03-493b-a1b8-c24e67ee5f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "db_refinement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
